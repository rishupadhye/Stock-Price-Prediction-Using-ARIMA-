{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Price Prediction \n",
    "Here we will focus on analyzing the prices of S&P500, Dow Jones Industrial Average and Gold over the period of 20 years from 01/01/2001 - 12/31/2020. We have built a time series predictive model below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import iplot\n",
    "import plotly.graph_objs as go\n",
    "import yfinance as yf\n",
    "import statsmodels.api as sm\n",
    "from datetime import timedelta\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from statsmodels.tsa.api import VAR\n",
    "from statsmodels.tsa.stattools import coint\n",
    "import warnings\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data =[]\n",
    "#Storing the data imported \n",
    "asset_ticker = ['GC=F','^GSPC','DJI']\n",
    "#List of Ticker Symbols\n",
    "asset_names = ['Gold','S&P 500','Dow_Jones Industrial Average']\n",
    "#List of Names \n",
    "\n",
    "for ticker in asset_ticker:\n",
    "    ft = yf.Ticker(ticker)\n",
    "    df = ft.history(start='2001-01-01',end='2020-12-30')['Open']\n",
    "    data.append(df)\n",
    "#Fetching the Stock prices from Yahoo finance \n",
    "\n",
    "price = pd.concat([df for df in data],axis=1)\n",
    "\n",
    "price.columns = asset_names \n",
    "#We will use actual names instead of symbols for column names\n",
    "\n",
    "price.tail(3)\n",
    "#Printing the top three rows "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPLORATORY DATA ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Handaling the Missing Values\n",
    "price.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in price.columns:\n",
    "    null_dt = price[price[column].isnull()].index \n",
    "    # find out date where it is null\n",
    "    idx = np.searchsorted(price.index, null_dt)\n",
    "    for i in range(len(idx)-1):\n",
    "        if math.isnan(price.iloc[idx[i]+1][column]): \n",
    "            # if value afterwards is NaN, then average will also be NaN\n",
    "            price[column].loc[null_dt[i]] = price.iloc[idx[i]-1][column] \n",
    "        elif math.isnan(price.iloc[idx[i]-1][column]):\n",
    "            price[column].loc[null_dt[i]] = price.iloc[idx[i]+1][column] \n",
    "            # if value before is NaN, then average will also be NaN\n",
    "        else:\n",
    "            avg = (price.iloc[idx[i]+1][column] + price.iloc[idx[i]-1][column])/2\n",
    "            price[column].loc[null_dt[i]] = avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly_price_open = price.resample('1Y').first()\n",
    "yearly_price_close = price.resample('1Y').last()\n",
    "yearly_price_high = price.resample('1Y').max()\n",
    "yearly_price_low = price.resample('1Y').min()\n",
    "\n",
    "#Using Open High Low Close Chart for analysis\n",
    "for column in price.columns:\n",
    "    fig = (go.Figure(data=go.Ohlc(\n",
    "                                    x=yearly_price_open[column].index.year,\n",
    "                                    open=yearly_price_open[column],\n",
    "                                    high=yearly_price_high[column],\n",
    "                                    low=yearly_price_low[column],\n",
    "                                    close=yearly_price_close[column])))\n",
    "    fig.update_layout(title_text=column,\n",
    "                  title={\n",
    "                    'y':0.85,\n",
    "                    'x':0.5,\n",
    "                    'xanchor': 'center',\n",
    "                    'yanchor': 'top'},)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exploring yearly rates \n",
    "def percent(df): \n",
    "    # function for transforming float DataFrame to DataFrame with percentages\n",
    "    for column in df.columns:\n",
    "        df[column] = df[column].apply(lambda x: \"{:.2%}\".format(x))\n",
    "    return df\n",
    "\n",
    "yearly_rates = (yearly_price_close - yearly_price_open)/yearly_price_open\n",
    "yearly_rates_p = percent(yearly_rates.copy())\n",
    "yearly_rates_p.index = yearly_price_open.index.year\n",
    "yearly_rates_p.tail(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent(yearly_rates.describe().drop('count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exploring Intercorrelations and Distribution of weekly prices \n",
    "# print(price)\n",
    "weekly_price = price.asfreq('W', method='ffill')\n",
    "# Ommitting Nan values and fetching the weekly value of last day\n",
    "# print(weekly_price)\n",
    "weekly_rates = ((weekly_price - weekly_price.shift(1))/weekly_price.shift(1)).iloc[1::,:]\n",
    "#Obtaining the weekly rates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(weekly_price.corr(), annot=True,linecolor=\"red\",fmt= '.1f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost perfect linear correlation is present between S&P500 and DJIA, but gold shows low values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(weekly_rates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe a same situation in the Covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before getting into the modeling part, let us check stationarity for both prices and rates of the corresponding assets.\n",
    "weekly_price.plot(subplots=True, layout=(4,2),figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_rates.plot(subplots=True, layout=(4,2), figsize=(10,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is obvious from the graph that prices show the presence of the upward trend, while rates indicate high possibility for stationarity. Since for the modeling part we need stationary data, I'll test the stationarity of the first difference data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performing AD Fuller Test\n",
    "def Stationarity(data, alpha=0.05):\n",
    "    adf = pd.DataFrame(columns = ['test','p'])\n",
    "    for col in data.columns:\n",
    "        df = adfuller(data[col], autolag='AIC')\n",
    "        adf = pd.concat([adf, pd.DataFrame(np.array(df[0:2]).reshape(1,2),columns = ['test','p'])],axis=0)\n",
    "    adf['Stationary'] = adf['p'] < alpha \n",
    "    # significance level\n",
    "    adf.index = data.columns\n",
    "    return adf\n",
    "# test statistic is lower than the critical value shown, you reject the null hypothesis and \n",
    "# infer that the time series is stationary.    \n",
    "weekly_price_d = weekly_price.diff().dropna()\n",
    "\n",
    "Stationarity(weekly_price_d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARIMA MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the procedure is the same with all our variables, I've decided to showcase the steps only for gold and the analog approach can be transferred to S&P500 and DJIA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Order Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is selecting the appropriate order of the ARIMA model; more accurately, we need to find how many past values (p) and moving averages (q) should be included in the model. Please note that I'm using the first difference of the dataset, so basically, the d parameter is equal to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2, figsize =(15,6))\n",
    "\n",
    "\n",
    "plot_acf(weekly_price_d['Gold'], lags=10, ax=axs[0], title='Gold'+': Autocorrelation')\n",
    "plt.close(2)\n",
    "plot_pacf(weekly_price_d['Gold'], lags=10, ax=axs[1], title='Gold'+': Partial Autocorrelation')\n",
    "plt.close(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we determine the nature of the auto-correlations we use the following rules of thumb.\n",
    "\n",
    "Rule 1: If the ACF shows exponential decay, the PACF has a spike at lag 1, and no correlation for other lags, then use one autoregressive (p)parameter\n",
    "\n",
    "Rule 2: If the ACF shows a sine-wave shape pattern or a set of exponential decays, the PACF has spikes at lags 1 and 2, and no correlation for other lags, the use two autoregressive (p) parameters\n",
    "\n",
    "Rule 3: If the ACF has a spike at lag 1, no correlation for other lags, and the PACF damps out exponentially, then use one moving average (q) parameter.\n",
    "\n",
    "Rule 4: If the ACF has spikes at lags 1 and 2, no correlation for other lags, and the PACF has a sine-wave shape pattern or a set of exponential decays, then use two moving average (q) parameter.\n",
    "\n",
    "Rule 5: If the ACF shows exponential decay starting at lag 1, and the PACF shows exponential decay starting at lag 1, then use one autoregressive (p) and one moving average (q) parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For order selection, ACF and PACF graphs were used and the value for both p and q is two, which means I'll use ARIMA(2,1,2) model for the actual dataset or ARIMA(2,0,2) for the first difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 40\n",
    "train_arima, test_arima = weekly_price_d['Gold'][:-test_size], weekly_price_d['Gold'][-test_size:]\n",
    "model_arima = ARIMA(train_arima, order=(2,0,2)) \n",
    "# d component is 0, so this would actually call ARMA model\n",
    "model_arima_fitted = model_arima.fit()\n",
    "model_arima_fitted.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_arima = model_arima_fitted.forecast(steps=test_size)[0]\n",
    "df_forecast_arima = pd.DataFrame(forecast_arima, index=weekly_price_d.index[-test_size:], columns=['Gold'] )\n",
    "\n",
    "print('Error:',np.sqrt(mean_squared_error(test_arima, df_forecast_arima)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets lower the error by scaling \n",
    "\n",
    "error = []\n",
    "for scaler in (MinMaxScaler(), StandardScaler()): # will use stnadard scaler and minMax sclaer\n",
    "    train_arima_scaled = scaler.fit_transform(train_arima.values.reshape(-1,1))\n",
    "\n",
    "    model_arima_scaled = ARIMA(train_arima_scaled, order=(2,0,2))\n",
    "    model_arima_scaled_fitted = model_arima_scaled.fit()\n",
    "    forecast_arima_scaled = model_arima_scaled_fitted.forecast(steps=test_size)[0]\n",
    "    forecast_arima_reverse = scaler.inverse_transform(forecast_arima_scaled.reshape(-1,1))\n",
    "\n",
    "    df_forecast_arima = pd.DataFrame(forecast_arima_reverse, index=weekly_price_d.index[-test_size:], columns=['Gold'])\n",
    "    error.append(np.sqrt(mean_squared_error(test_arima, df_forecast_arima)))\n",
    "\n",
    "pd.DataFrame(error, index = ['MinMaxScaler', 'StandardScaler'], columns = ['Error'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error is almost the same so there is no point of scaling the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_diff(df, df_forecast):\n",
    "    last_row = df.iloc[-1,:]\n",
    "    l = []\n",
    "    num_forecast = df_forecast.shape[0]\n",
    "    for row in range(num_forecast):\n",
    "        arr = np.array(last_row + df_forecast.iloc[row,:])\n",
    "        l.append(arr)\n",
    "        last_row = arr\n",
    "#     print(df.index[-1])\n",
    "#     print(df.index[-1]+timedelta(days=2))\n",
    "    time = pd.date_range((df.index[-1]+timedelta(days=2)).date(), num_forecast, freq='W')\n",
    "#     print(df_forecast.index)\n",
    "#     print(df_forecast.index + timedelta(days = 28))\n",
    "    result = pd.DataFrame(data=l, columns=df.columns, index=(df_forecast.index+timedelta(days = 28)))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_arima = ARIMA(weekly_price_d['Gold'], order=(2,0,2)) \n",
    "model_arima_fitted = model_arima.fit()\n",
    "prediction_size = 4 # next month\n",
    "forecast_arima = model_arima_fitted.forecast(steps=prediction_size)[0]\n",
    "# print(weekly_price_d.index[-prediction_size:])\n",
    "df_forecast_arima = pd.DataFrame(forecast_arima, index=weekly_price_d.index[-prediction_size:], columns=['Gold'] )\n",
    "# print(\"DF Forecast ARIMA:\",df_forecast_arima)\n",
    "weekly_price_forecast_arima = inverse_diff(pd.DataFrame(weekly_price['Gold']), df_forecast_arima)\n",
    "# weekly_price_forecast_arima.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df_forecast_arima)\n",
    "weekly_price_forecast_arima = inverse_diff(pd.DataFrame(weekly_price['Gold']), df_forecast_arima)\n",
    "weekly_price_forecast_arima.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
